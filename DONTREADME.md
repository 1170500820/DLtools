# DLTools

当前开发目标：

一个至少**可用**的版本，保留**扩展**的空间，其他功能后续迭代时加入。

如何进行自动的输入/输出键值检查？

#### 指令速查

##### 开启一个JointEE模型的训练进程

```shell
python run_train.py --epoch 21 --save_epoch 5 --bsz 16 --eval_freq_batch 300 --eval_freq_epoch 1 --eval_start_batch 100 --eval_start_epoch 1 --print_info_freq 100 --dir work.PLMEE.joint + --train_file data/NLP/EventExtraction/FewFC-main/train.json --val_file data/NLP/EventExtraction/FewFC-main/val.json
```



```shell
python run_train.py --epoch 21 --save_epoch 5 --bsz 16 --eval_freq_batch 300 --eval_freq_epoch 1 --eval_start_batch 100 --eval_start_epoch 1 --print_info_freq 100 --dir work.PLMEE.event_detection + --train_file data/NLP/EventExtraction/FewFC-main/train.json --val_file data/NLP/EventExtraction/FewFC-main/val.json
```



## 目录

- **analysis**

  分析数据、模型、模型结果、评价指标、模型中间状态等等。总之是提供一系列封装好方便调用的分析工具。

- **data**

  存放一些常用的数据。

- **evaluate**

  各类评价函数，评价器。该目录存在的目的就是能够更好的构建Evaluator

- **models**

  存放一些自建的基础模型。在开发过程中发现的可复用结构，或者各类用途的ToyNet，都存放在该目录下，这是DLtools的工具属性

- **process**

  数据处理框架。

- **test**

- **train**

  训练流程相关的代码。提供model、loss等，执行完整的训练、评价等流程。

- **utils**

- **work**

  存放模型/数据等与特定任务相关的目录。在运行时指定该目录，会在该目录下的代码与数据进行操作。通过目录可以隔离不同的模型。同时任务与DLtools的基本代码无关，达到可插拔的效果。

## 设计思路

### 总体设计思路

一句话概括：设计一个框架，能进行函数之间的运算，从而复用函数搭建复杂的处理流程。

#### 一个简单的例子

现在有数据D = {d_1, d_2, ..., d_n}与R = {r_1, r_2, ..., r_m}，过程P1满足P1(D) =  R。

另有数据S = {s_1, s_2, ..., s_k}，过程P2满足P2(R) = S。

我希望能够在P上定义运算，比如说加法：P3 = P1 + P2, P3(D) = S

这个是通过加法定义了P的顺序执行。我还可以用乘法定义P的并列执行。

加法的引入是符合直觉的，但是乘法不那么符合直觉，只不过我发现将乘法定义为“并列”之后，能通过加法与乘法的组合，得到任意（？）Process的处理流



#### 这是函数式编程吗？

函数式编程是函数可以作为另一个函数的变量。那么在函数上定义运算，是否已经有对应的编程语言，或者对应的概念呢？数学里倒是有一个对应的概念：泛函分析。但是我简单翻阅了wiki，我觉得泛函分析主要讨论的函数空间性质，和我想解决的问题不是很重合。

我只想精确定义函数之间的运算，然后通过积累通用函数，搭建各种复杂模块，通过不断泛化，最终简化数据处理流程——就像搭积木一样。



#### 我的第一个想法

现在也还在完成中的，是设计DataProcessor与ProcessorManager，这两个类勉强实现了上面所说的功能：

1，**加法得到过程的串联**
2，**乘法得到过程的并联**

在我的实现中，遇到的一个障碍是，如何让Processor知道传入的参数，哪个是哪个？因为参数一般是很多的。

最早的实现，Processor只是一批默认输入与输出都是dict的Callable。我只要把他们放在一个list里面，按顺序调用即可，dict的key就指明了参数。
但是这需要实现就定好数据，比如P1是把{id, content, trigger}变成{id, tokenized, match, label}， 然后P2把{tokenized}变成{tensor}
这种顺序结构有两个问题：

1. **参数必须完全对齐**。

   假设P1，P2，P3是三个连续的Processor，R在P1中需要，在P3中也需要，那我如何传入？
   如果我让P接收参数的时候不把参数吃掉，那么ok，P3也可以接收R。如果P1需要修改R，返回一个新的R呢？
   这个时候产生了“全局变量”的概念，全局变量就是，每个P都能够看到的。而局部变量就是P之间的那个域里面的变量。
   如果在数学的function中，总会有些常数之类的东西，把这个类比为全局变量，确实可以。只不过加入全局变量有点麻烦，这个是后话了，我之前否决了这个方案，现在写到这里突然觉得好像确实有其合理性

2. **参数写死了**。
   如果我有另一套数据，刚好和现在这套数据结构差不多，有许多P可以复用，但是复用我就必须重新设置input_keys和output_keys
   这样的复用说实话，不是不能用，但是不符合我对这个框架的期待：我觉得他的复用应该更简洁的，最好是拿来就用，或者自动分析。



#### 面临的主要困难

就是两个

1. 如何让P更方便泛化
2. 如果让P之间的参数对齐更方便。



#### 如果使用typing提供的类型呢



描述一个数据个体，data_unit

一个data_unit拥有一个类型以及一个名字，这个与编程语言中的变量的概念是一致的。

这样，一个data_unit就可以表示为(t, s)，t为类型，s为名字。



一个data_processor的输入与输出的都是多个data_unit。



匹配？

将情况简化为两个data_processor前后串接，现在要讨论如何将n个来自上一个data_processor的output的data_unit与m个下一个data_processor的input的data_unit进行匹配。

注意这里m与n是不一定相等的，因为我希望提供足够宽泛的输入输出的匹配以最大程度的简化data_processor的设计。这个也是可选的，也可以强行让m=n，不过这是后话了。

匹配中，名字为第一优先级，类型为第二优先级。

算法的粗略描述是这样：

首先，input与output中，各自不允许出现重复的名字。

然后，将input与output中，名字相同的data_unit，自动建立匹配。

接着，对于剩下的未构成匹配的data_unit，按照类型匹配

​	这个是先按照严格类型相等，再按照子类型相等。

最后，当类型与名字都无法继续找到匹配时，对于剩下的输入与输出，如果数量相同，则直接按顺序匹配。



这个算法需要更仔细的考虑。

#### In-dict与序列处理模式

我尝试过不少数据处理模式来使数据处理更加方便，in-dict是一个较好的模式。

##### "失败"的尝试

最开始是直接一个数据一个数据的处理，没有任何章法。这样的结果就是完全无法扩展的代码。一个数据生成写成一个四五百行的巨无霸function，每添加或者删除一个数据处理步骤都非常痛苦，就更难进行模型的高速迭代了。



后来我专门给数据建立了一个类，然后写一些类方法来自动处理。通过面对对象来使数据处理过程变简单的想法完全不行，结果是数据处理没有简化，反而还要花费精力在维护类的内部状态上。而且换一个数据就要写一个完全不同的类，完全没有可扩展性。



再后来我是先意识到深度学习代码会涉及很多需要同步处理的数据，比如data1, data2, data3, label1, label2，需要同步处理。因此使用python的解包等语法糖可以使代码更加整洁。这个给扩展性带来了一定的帮助，但是治标不治本。



然后就是尝试使用DataProcessor。最初的尝试，DP是完全写死的，每一个DP的输入输出的key值完全确定，而且只写在注释里，不会专门开辟一个数据结构去存储input_keys和output_keys。

DP用dict来组织数据，每个DP执行一个单独的功能。这个方法带来了一定的方便，数据处理中通用的步骤确实能够直接复用了。但是到后面数据处理过程复杂，首先是DP写死key值本身就导致其很难复用了，然后时间紧迫我也没有仔细研究模块化的技巧，导致后期DP的粒度都过大，很难拆开，复用性就不复存在了。

不过这个尝试本身有两点发现：一是通过模块复用确实有用，但是要更细的复用性，DP写死输入输出的特性必须改变，必须更灵活。二就是用dict保存数据确实很方便。



腾讯的实习中我发现了typing的妙处，以及严格定义类型和积累基本接口方法的好处，我开始重新设计DataProcessor。一开始DataProcessor的input_keys与output_keys保存为对象的属性，但是可以访问却依旧难以复用，因为参数是字符串，还是写死的。

后来我尝试引入类型，引入参数匹配，这个是目前为止的最后尝试了。引入类型是为了让参数能够通过类型来确定对应关系，比如一个模块只需要接受一个字符串和一个Dict[int, list]的参数，那么另一个模块也有同样类型的两个输出，二者就能直接对应了。关于可拼接函数的想法在前面也写了不少所以这里不再赘述。

一个简短的总结：参数配对算法是可拼接函数这个想法成功的核心问题，参数的形式反而次之。现在我设计的参数配对算法限制太多，很多情况都没法成功匹配，为了规避这些设计缺陷，我就不得不在搭建可拼接函数组成的计算流中设计各种很不好的结构。结果大部分时间用在这个上面了，反而写具体有效的代码的时间反而少了。



**DP可拼接函数，大量复用，由基础组件组合成复杂框架，不断积累。这个方向是可行的，但是需要足够好用的参数匹配算法，否则开发组件的负担过大**



现在我打算还是多写一些工具函数为好。



in-dict与序列处理模式，简而言之就是两个关键步骤

- 数据用dict保管
- 处理函数输入data_dict，输出List[data_dict]，可以进行修改、删除、增加。

### 具体模块的设计思路

#### 自定义模型

自己实现的针对特定任务的模型就是自定义模型。DLtools希望自定义模型是可插拔的，尽量让DLtools不去依赖特定的模型结构，而是提供通用的框架，特定结构都放在特定模型内部去实现，这样方便DLtools独立更新，自定义模型也能独立的改进，不太需要同时考虑两边的影响。

在run_train.py开启一个训练时，需要指定自定义模型的模块地址，然后需要自定义模块提供下面的东西

- **model** nn.Module本体
- **lossFunc** 损失函数模块
- (Opt.)**evaluator** 评价函数模块
- **optimizers** 优化器的列表
- **train_data** 训练数据的工厂
- (Opt.)**val_data** 测试数据的工厂
- (Opt.)**use** 可以方便调用的模块，可用于预测与部署

模型的命名可能五花八门，而且有可能是多个模块组合起来的，所以需要告诉run_train.py具体哪个是哪个，而不是让run_train.py自己找。所以约定好在自定义模块中需要包含一个dict：model_registry

```python
model_registry = {
    "model": BertSimilarity,
    "args": [],  # List[Args]
    "loss": BertSimilarityLoss,
    "evaluator": BertSimilarityEvaluator,
    "train_data": dataset_factory_train,
    "val_data": dataset_factory_val,
  	"use": UseModel
}
```

其中args是需要用到的参数。run_train.py会为这些参数创建argparser，然后合并到主argparser中。run_train.py会把所有这些提供的项都当成工厂，调用他们来获得model，evaluator等的对象本体。如果有参数，会从通用参数和特有参数中寻找匹配值，然后用\*\*kw传入。

##### use模块中的数据处理

use模块的构成通常很简单，就是val_dataset生成的过程再去掉生成label，即只会将源格式数据转化为model(eval)下的输入格式，然后调用模型，再将输出转化为源格式。

输入模型之后的部分很简单，主要是数据处理这一部分。我需要单独从val_dataset把要用到的部分拿出来，放在useModel里面

我觉得这部分是完全可以复用的。更进一步，整个模型py文件里面，数据处理部分应当都是可以复用的。从源文件到train input，eval input，train label，eval gt，模型的输出到源格式，以及各种各样别的格式。

数据处理不好定义那么整齐的接口，因为数据处理本身可能就是多阶段的，多种格式的。



也许可以不用为数据处理的整个过程定义统一的接口，只要提高复用程度，就好

#### run_*.py｜程序入口

整个DLtools的调用入口都是命令行。

##### 目标

- 一个run.py解决所有问题
- 可交互的程序，可多次输入指令，甚至有运行时环境（比如说魔改ipython）

但是能力所限，处理train，eval，analysis，以及各种不同情况，不同模型，不同数据的各类命令的分支，都过于繁琐了，而且我也没有找到很好的组织方法。

现在的实现是沿用以前的，按功能分。比如说run_train.py就是用来跑训练模型的，run_analysis.py就是专门分析数据结果。

##### run_train.py

run_train.py将训练通用参数与模型特有参数分离，这样在训练不同模型的时候不用修改run部分的代码了。用“+”分隔，约定在写模型代码的时候要提供所需的参数，run_train.py会把读取到的参数传入，然后由模型端来接收，并自由处理这些参数。

#### analysis

对数据、模型、结果等进行分析。

抽象起来就是对数据进行分析，因为模型对中间状态，模型的结果这些也都是数据。但是这不是analysis的全部：一部分analysis还是需要嵌入到train，eval等过程之中的。

#### DataProcessor

一个将一些数据转化为另一些数据的处理过程。

以dict作为输入与输出，key代表数据的类型，value则是对应的值。

##### ADT的细节

- 不允许输入与输出的keys为空。即至少要有一个输入和一个输出。

##### 在考虑的特性

输入的数据中，键值有**隐式顺序**，如果提供的数据没有被用dict包装，那么自动包装后，将按照输入的顺序进行。（这个应该是锦上添花的扩展功能，提供方便的。现在应该抓住主要矛盾）

- 输入通配符
- 隐式顺序
- 从函数创建
- 输入的自动包装
- 超级简易的类型系统
  - 没有继承，只有相同类型合并。



#### ProcessorManager

管理DataProcessor的处理流。

#### Recorder

recorder.py在analysis目录下面，因为从分类来说，recorder就是为了分析而存在的工具。本来是应该和analysis放在一起写，但是确实又有区别，就分开吧。

recorder通过在训练、测试一个模型的过程中的各个部分接入hook，来提供从训练的几乎所有阶段、所有模块获取信息的能力。

##### 设计方向

###### record的格式

record是Recorder的内部数据结构。在目前的设计，每一个Recorder都只通过record存储与模型相关的数据。

record本身是一个Dict[str,Any]格式的字典，我的想法是，str指明数据是啥，然后Any可能具有各种奇怪的结构，对各种结构分别实现各类分析或者可视化的function放在analysis目录下，按需调用。

不过record里面要是加一些固定字段，可以有更多玩法：

- 时间，序号类

  加入时间、序号，最终的目的还是用于标定这个数据是什么时候生成的。

- 参数标识

  这个很泛，就是用该模型的某一个或几个关键参数来标识这个record结果。这个标识是可以这样用：加入我多次训练生成了很多个record文件，我可以通过函数来组织各种含有关键参数的，然后同时对他们进行分析，就像sql查询字段一样。在这个想法上进行多次训练、不同参数、不同模型之间的平行比较，要有条理很多——以前的想法都是直接存一个大表，但是那是从sql的角度想的，sql数据都在，可以任意组织，而我这个是数据生成很难的，因此先把数据按一个框架组织好，然后定义一些规整的操作和规约，在这些的基础上去做更复杂的对比分析，要清晰很多，可维护性也会好。

